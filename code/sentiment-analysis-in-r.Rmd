---
title: "Sentiment Analysis in R"
author: "Jenn Schilling"
date: "9/16/2021"
output: html_document
---

# Introduction

This file contains the code for webinar: [Sentiment Analysis in R](https://www.airweb.org/collaborate-learn/calendar/2021/10/20/event/sentiment-analysis-in-r), presented for the Association for Institutional Research, October 20 & 22 2021.  

**Webinar Details**  
This webinar will teach participants how to complete a text analysis in R, including data processing and cleaning, visualization of word frequencies, and sentiment analysis. Sentiment analysis is useful for finding patterns in text data from open response questions on surveys and course evaluations as well as evaluating social media posts. This series is ideal for higher education professionals who have some experience in R and want to add text analysis to their R skills.

As a result of this webinar, participants will be able to: 
- Prepare data for a text analysis in R. 
- Conduct text mining in R.
- Complete sentiment analysis in R. 

**Materials developed by Jenn Schilling.**  

# Setup

Load libraries and tweet data.

```{r setup, message = FALSE}

library(here) # working directory
library(tidyverse) # data processing and plotting
library(tidytext) # text analysis

# Read data
text_data <- read_csv(here("data", "uarizona_tweets.csv"), show_col_types = FALSE)

```

# Data Processing

The first step is to understand and process the data. This particular dataset is a subset of what is pulled from the `rtweet` package. The full data includes more details about the tweet and engagement with it, but this subset includes the date, user, tweet text, source, and a few other metrics that may be of interest. 

Now maybe you do not want to look at tweets, the same process we are going to walk through in this webinar could be used for any type of text data. I pulled Twitter data to use publicly accessible data that is relevant to my institution, using hashtags that are related to my university, so that I would have a good demonstration dataset. But this same process would work with any text dataset, you would just adjust the code to read the data in the "Setup" chunk above.

One important part of text analysis is having an identification column. Since we will eventually be creating a data frame of individual words, we want to be able to tie those words back to the original text (in this case, tweet). Having an identification column allows us to tie back to the original text. In this case, we will use the `status_id` field to identify each tweet.

```{r view-data}

# First let's look at the data
View(text_data)

# Check the number of rows and columns
dim(text_data)

# View the data types of the columns
glimpse(text_data)

```

Once we have a basic understanding of the data, we need to process it. We will first make all the text lowercase. Then we need to expand contractions, remove special characters and emojis, and remove extra whitespace. We also want to make the `status_id` column a character column instead of a numeric column.

```{r process-data}

text_data_processed <- text_data %>%
  
# Lowercase
  mutate(text = str_to_lower(text)) %>%
  
# Expand contractions
  mutate(text = gsub("n't|n’t", " not", text),
         text = gsub("'ll|’ll", " will", text),
         text = gsub("'re|’re", " are", text),
         text = gsub("'ve|’ve", " have", text),
         text = gsub("'m|’m", " am", text),
         text = gsub("'d|’d", " would", text),
         text = gsub("it's|it’s", "it is", text), 
         text = gsub("'s|’s", "", text)) %>%
  
# Remove emojis
  mutate(text = gsub("\U0001", "", text)) %>%
  
# Remove links
  mutate(text = gsub("(https:|http:).*", "", text)) %>%
  
# Remove special characters
  mutate(text =  gsub("[^a-zA-Z0-9 ]", " ", text)) %>%
  
# Remove extra whitespace
  mutate(text = str_squish(text)) %>%
  
# Make identification column a character
  mutate(status_id = as.character(status_id))

```

Now that the text processing is complete, let's take another look at the data.

```{r check-data}

# View the columns and a few records
glimpse(text_data_processed)

# Plot number of tweets over time
tweets_time <- text_data_processed %>%
  mutate(created_at_date = as.Date(created_at, "%m/%d/%Y")) %>%
  group_by(created_at_date) %>%
  summarise(n = n(),
            .groups = "drop")

ggplot(data = tweets_time,
       mapping = aes(x = created_at_date,
                     y = n,
                     group = 1)) +
  geom_line() +
  scale_x_continuous(breaks = seq(min(tweets_time$created_at_date),
                                  max(tweets_time$created_at_date),
                                  "weeks")) +
  labs(x = "Date",
       y = "Number of Tweets") +
  theme_classic()


# Plot number of screen names over time
tweets_user <- text_data_processed %>%
  mutate(created_at_date = as.Date(created_at, "%m/%d/%Y")) %>%
  select(created_at_date, screen_name) %>%
  unique(.) %>%
  group_by(created_at_date) %>%
  summarise(n = n(),
            .groups = "drop")

ggplot(data = tweets_user,
       mapping = aes(x = created_at_date,
                     y = n,
                     group = 1)) +
  geom_line() +
  scale_x_continuous(breaks = seq(min(tweets_user$created_at_date),
                                  max(tweets_user$created_at_date),
                                  "weeks")) +
  labs(x = "Date",
       y = "Number of Users") +
  theme_classic()

# Plot histograms of favorite count and retweet count
ggplot(data = text_data_processed,
       mapping = aes(x = favorite_count)) +
  geom_histogram(bins = 10) +
  labs(x = "Favorite Count",
       y = "Count") +
  theme_classic()

ggplot(data = text_data_processed,
       mapping = aes(x = retweet_count)) +
  geom_histogram(bins = 10) +
  labs(x = "Retweet Count",
       y = "Count") +
  theme_classic()


```


# Tokenizing

After processing the data, the next step in a text analysis is to get individual words. This is called tokenizing. Tokenizing involves splitting a long text string, such as a tweet, document, or open-ended responses, into individual words or sets of words. A token is a meaningful unit of text, such as a word. For text analysis, we can create a tidy text data table or data frame which contains one token per row. 

There is a simple function in the `tidytext` that will  complete the tokenizing process for us called `unnest_tokens()`. This will create a new data frame with one row for each word in the text and a new column with each word.

After tokenizing, we will remove stop words, which are common words in the English language that are not useful for text analysis. These are words such as "and", "the", "can", "a", etc.

```{r}

# Tokenize the text data to get each individual word
text_tokens <- text_data_processed %>%
  unnest_tokens(word, text)

# Let's see what the new table looks like
View(text_tokens)

dim(text_tokens)

# Let's take a look at the stop words list
View(stop_words)

# Now remove the stop words
text_tokens <- anti_join(text_tokens, stop_words, by = "word")

# Look at the tokenized data frame again
View(text_tokens)

dim(text_tokens) # notice the drop in the row count now that stop words are gone

```


# Word Frequencies

# Sentiment Analysis

# Visualizing Results

# Moving Beyond Words
